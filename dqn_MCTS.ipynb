{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment lux_ai_s2 failed: No module named 'vec_noise'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elian Morel\\.conda\\envs\\tetris\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import make\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from Scripts.MCTS import agent_mcts\n",
    "from Scripts.Deep_Q_Learning import DQN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 7\n",
    "rows = 6\n",
    "env = make(\"connectx\", configuration={\"rows\":rows, \"columns\":cols})\n",
    "sync_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 100\n",
    "observation_space = gym.spaces.Discrete(cols * rows).n\n",
    "action_space = gym.spaces.Discrete(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_agent(obs,config):\n",
    "    global load_network\n",
    "    agent =DQN()\n",
    "    agent.network = load_network\n",
    "    print(load_network)\n",
    "    return agent.choose_action(obs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elian Morel\\AppData\\Local\\Temp\\ipykernel_23468\\1774338429.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 Average Reward -19.6 Best Reward 1 Last Reward 1 Epsilon 0.9531108968798944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\05-Elian\\04-Polytechnique\\03-3A\\10- Advanced Machine Learning\\projet\\Scripts\\Deep_Q_Learning.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(observation).float().detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 Average Reward -14.95 Best Reward 1 Last Reward -1 Epsilon 0.8658253647948594\n",
      "Episode 30 Average Reward -29.9 Best Reward 1 Last Reward -100 Epsilon 0.7833919908382508\n",
      "Episode 40 Average Reward -29.9 Best Reward 1 Last Reward -100 Epsilon 0.7173681503955072\n",
      "Episode 50 Average Reward -27.84 Best Reward 1 Last Reward 1 Epsilon 0.6635141250307047\n",
      "Episode 60 Average Reward -31.55 Best Reward 1 Last Reward -1 Epsilon 0.6009433131399452\n",
      "Episode 70 Average Reward -32.75714285714286 Best Reward 1 Last Reward 1 Epsilon 0.5448178815347047\n",
      "Episode 80 Average Reward -31.1375 Best Reward 1 Last Reward 1 Epsilon 0.48901714023696286\n",
      "Episode 90 Average Reward -30.977777777777778 Best Reward 1 Last Reward -1 Epsilon 0.4514016473420717\n",
      "Episode 100 Average Reward -28.83 Best Reward 1 Last Reward 1 Epsilon 0.41584661836923925\n",
      "Episode 110 Average Reward -27.990909090909092 Best Reward 1 Last Reward -100 Epsilon 0.3827090189332178\n",
      "Episode 120 Average Reward -28.133333333333333 Best Reward 1 Last Reward -100 Epsilon 0.34731301240586104\n",
      "Episode 130 Average Reward -28.26923076923077 Best Reward 1 Last Reward -100 Epsilon 0.31867870398971\n",
      "Episode 140 Average Reward -28.385714285714286 Best Reward 1 Last Reward 1 Epsilon 0.28833798882226386\n",
      "Episode 150 Average Reward -27.126666666666665 Best Reward 1 Last Reward 1 Epsilon 0.266158857095684\n",
      "Episode 160 Average Reward -26.63125 Best Reward 1 Last Reward 1 Epsilon 0.24691788022723601\n",
      "Episode 170 Average Reward -26.823529411764707 Best Reward 1 Last Reward -1 Epsilon 0.22363299200356998\n",
      "Episode 180 Average Reward -26.433333333333334 Best Reward 1 Last Reward -1 Epsilon 0.20173495769715546\n",
      "Episode 190 Average Reward -26.073684210526316 Best Reward 1 Last Reward -100 Epsilon 0.18307688713652753\n",
      "Episode 200 Average Reward -25.76 Best Reward 1 Last Reward -100 Epsilon 0.1674796218391289\n",
      "Episode 210 Average Reward -24.995238095238093 Best Reward 1 Last Reward -1 Epsilon 0.15506170857294269\n",
      "Episode 220 Average Reward -24.29090909090909 Best Reward 1 Last Reward -1 Epsilon 0.14100221871554078\n",
      "Episode 230 Average Reward -24.517391304347825 Best Reward 1 Last Reward 1 Epsilon 0.12668733098884255\n",
      "Episode 240 Average Reward -23.9 Best Reward 1 Last Reward 1 Epsilon 0.11635892113259806\n",
      "Episode 250 Average Reward -23.332 Best Reward 1 Last Reward 1 Epsilon 0.10570281555543207\n",
      "Episode 260 Average Reward -23.196153846153845 Best Reward 1 Last Reward 1 Epsilon 0.09621492569550721\n",
      "Episode 270 Average Reward -22.703703703703702 Best Reward 1 Last Reward -1 Epsilon 0.08740359953047568\n",
      "Episode 280 Average Reward -22.6 Best Reward 1 Last Reward -100 Epsilon 0.07900300839416118\n",
      "Episode 290 Average Reward -21.813793103448276 Best Reward 1 Last Reward -1 Epsilon 0.07241709596708645\n",
      "Episode 300 Average Reward -21.41 Best Reward 1 Last Reward -100 Epsilon 0.06604896579725783\n",
      "Episode 310 Average Reward -21.351612903225806 Best Reward 1 Last Reward -100 Epsilon 0.06006028701082751\n",
      "Episode 320 Average Reward -20.96875 Best Reward 1 Last Reward 1 Epsilon 0.05412503357177137\n",
      "Episode 330 Average Reward -20.615151515151513 Best Reward 1 Last Reward 1 Epsilon 0.04951384249760823\n",
      "Episode 340 Average Reward -20.3 Best Reward 1 Last Reward 1 Epsilon 0.04552266113410808\n",
      "Episode 350 Average Reward -20.002857142857142 Best Reward 1 Last Reward 1 Epsilon 0.041686036621223575\n",
      "Episode 360 Average Reward -19.72222222222222 Best Reward 1 Last Reward 1 Epsilon 0.0383641984178818\n",
      "Episode 370 Average Reward -20.25945945945946 Best Reward 1 Last Reward -100 Epsilon 0.03460759468259993\n",
      "Episode 380 Average Reward -20.513157894736842 Best Reward 1 Last Reward 1 Epsilon 0.03137539932255636\n",
      "Episode 390 Average Reward -20.235897435897435 Best Reward 1 Last Reward 1 Epsilon 0.028473550177962683\n",
      "Episode 400 Average Reward -19.715 Best Reward 1 Last Reward 1 Epsilon 0.02594370663721214\n",
      "Episode 410 Average Reward -19.234146341463415 Best Reward 1 Last Reward 1 Epsilon 0.023900228846246697\n",
      "Episode 420 Average Reward -19.247619047619047 Best Reward 1 Last Reward 1 Epsilon 0.021559930008775024\n",
      "Episode 430 Average Reward -19.02093023255814 Best Reward 1 Last Reward -1 Epsilon 0.019429343308326463\n",
      "Episode 440 Average Reward -19.03409090909091 Best Reward 1 Last Reward -100 Epsilon 0.01766767914446943\n",
      "Episode 450 Average Reward -19.266666666666666 Best Reward 1 Last Reward 1 Epsilon 0.01598557734764059\n",
      "Episode 460 Average Reward -19.27391304347826 Best Reward 1 Last Reward 1 Epsilon 0.014492595549727911\n",
      "Episode 470 Average Reward -19.06595744680851 Best Reward 1 Last Reward -100 Epsilon 0.013244638608616273\n",
      "Episode 480 Average Reward -19.077083333333334 Best Reward 1 Last Reward -100 Epsilon 0.012055799203603838\n",
      "Episode 490 Average Reward -18.877551020408163 Best Reward 1 Last Reward 1 Epsilon 0.010907992446600463\n",
      "Episode 500 Average Reward -18.69 Best Reward 1 Last Reward 1 Epsilon 0.009928890514618687\n",
      "Episode 510 Average Reward -18.307843137254903 Best Reward 1 Last Reward 1 Epsilon 0.009073914068699134\n",
      "Episode 520 Average Reward -17.948076923076922 Best Reward 1 Last Reward 1 Epsilon 0.0083008604777794\n",
      "Episode 530 Average Reward -17.78490566037736 Best Reward 1 Last Reward -100 Epsilon 0.007563338176165085\n",
      "Episode 540 Average Reward -17.81851851851852 Best Reward 1 Last Reward 1 Epsilon 0.006850099086846335\n",
      "Episode 550 Average Reward -17.854545454545455 Best Reward 1 Last Reward 1 Epsilon 0.006210330311021085\n",
      "Episode 560 Average Reward -17.525 Best Reward 1 Last Reward 1 Epsilon 0.005579842493468897\n",
      "Episode 570 Average Reward -17.387719298245614 Best Reward 1 Last Reward -1 Epsilon 0.005099362434107482\n",
      "Episode 580 Average Reward -17.248275862068965 Best Reward 1 Last Reward 1 Epsilon 0.004595435298052106\n",
      "Episode 590 Average Reward -16.952542372881357 Best Reward 1 Last Reward -1 Epsilon 0.004166242088180963\n",
      "Episode 600 Average Reward -16.66 Best Reward 1 Last Reward 1 Epsilon 0.0038304122083048253\n",
      "Episode 610 Average Reward -16.53934426229508 Best Reward 1 Last Reward 1 Epsilon 0.0035005755350691233\n",
      "Episode 620 Average Reward -16.266129032258064 Best Reward 1 Last Reward 1 Epsilon 0.003192746004225058\n",
      "Episode 630 Average Reward -16.001587301587303 Best Reward 1 Last Reward -1 Epsilon 0.0029265897398862324\n",
      "Episode 640 Average Reward -16.0609375 Best Reward 1 Last Reward -100 Epsilon 0.002647955414472615\n",
      "Episode 650 Average Reward -15.966153846153846 Best Reward 1 Last Reward 1 Epsilon 0.002417520118027434\n",
      "Episode 660 Average Reward -15.718181818181819 Best Reward 1 Last Reward -1 Epsilon 0.002196124548509679\n",
      "Episode 670 Average Reward -15.622388059701493 Best Reward 1 Last Reward 1 Epsilon 0.002001001303638341\n",
      "Episode 680 Average Reward -15.675 Best Reward 1 Last Reward 1 Epsilon 0.0018032591605341237\n",
      "Episode 690 Average Reward -15.58840579710145 Best Reward 1 Last Reward 1 Epsilon 0.0016381173750087503\n",
      "Episode 700 Average Reward -15.354285714285714 Best Reward 1 Last Reward 1 Epsilon 0.001510600287709845\n",
      "Episode 710 Average Reward -15.556338028169014 Best Reward 1 Last Reward -1 Epsilon 0.0013763851176087018\n",
      "Episode 720 Average Reward -15.620833333333334 Best Reward 1 Last Reward -100 Epsilon 0.0012629087013811618\n",
      "Episode 730 Average Reward -15.398630136986302 Best Reward 1 Last Reward 1 Epsilon 0.0011472520046255821\n",
      "Episode 740 Average Reward -15.321621621621622 Best Reward 1 Last Reward 1 Epsilon 0.0010463662951741403\n"
     ]
    }
   ],
   "source": [
    "trainer = env.train([None, agent_mcts])\n",
    "agent =DQN()\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "load_network = 0\n",
    "j=0\n",
    "flag = False\n",
    "EPISODES = 5000\n",
    "RANDOM_PLAYS = 1000\n",
    "UPDATE_NETWORK = 1000\n",
    "for i in range(1, EPISODES):\n",
    "    state, info = env.reset()\n",
    "    state = state[\"observation\"][\"board\"]\n",
    "    state = np.reshape(state, [1, rows, cols])\n",
    "    score = 0\n",
    "    if i < RANDOM_PLAYS:\n",
    "        trainer = env.train([None,\"random\"])\n",
    "        \n",
    "    else: \n",
    "        if(i %UPDATE_NETWORK == 0):\n",
    "            print(\"aaa\")\n",
    "            load_network = agent.network\n",
    "            trainer = env.train([None,dqn_agent])\n",
    "            agent.exploration_rate = 0.5\n",
    "            plt.plot(episode_number[-1000:],average_reward_number[-1000:])\n",
    "    if i%10000 == 0:\n",
    "        torch.save({'model_state_dict': agent.network.state_dict(),\n",
    "        'optimizer_state_dict': agent.network.optimizer.state_dict(),\n",
    "        'epoch': EPISODES},\"Reinforced-Connect4/models/network\" + str(i) + \".pkl\"\n",
    "        )\n",
    "    while True:\n",
    "        #print(j)\n",
    "        j+=1\n",
    "\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, info = trainer.step(action)\n",
    "        if reward == None :\n",
    "            reward = -100\n",
    "        state_ = state_[\"board\"]\n",
    "        state_ = np.reshape(state_, [1, rows, cols])\n",
    "        state = torch.tensor(state).float()\n",
    "        state_ = torch.tensor(state_).float()\n",
    "\n",
    "        #print(\"state1 : \", state)\n",
    "        #print(\"state2 : \", state_)\n",
    "        exp = (state, action, reward, state_, done)\n",
    "        agent.replay.add(exp)\n",
    "        agent.learn()\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if j % sync_freq == 0:\n",
    "            agent.network2.load_state_dict(agent.network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            if i%10==0:\n",
    "                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "                #test_model(agent,10, observation_space)\n",
    "            break\n",
    "  \n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n",
    "\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = env.train([None,agent_mcts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkB(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1536, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       "  (loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_games = 0\n",
    "history = {1:0, -1:0,0:0, None:0}\n",
    "env.reset()\n",
    "trainer = env.train([None,dqn_agent])\n",
    "while nb_games < 200:\n",
    "    #env.render()\n",
    "    \n",
    "    if done :\n",
    "        #print(np.reshape(observation[\"board\"], (6,7)))\n",
    "        observation, info = env.reset()\n",
    "        observation = observation[\"observation\"]\n",
    "        history[reward] += 1\n",
    "        nb_games +=1\n",
    "        \n",
    "    \n",
    "    action = agent.choose_action(np.reshape(observation[\"board\"], [1, rows,cols]))\n",
    "    observation, reward, done, info = trainer.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 16, -1: 22, None: 162}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of Network(\n",
       "  (fc1): Linear(in_features=42, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=7, bias=True)\n",
       "  (loss): MSELoss()\n",
       ")>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tetris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1de2aa2d83dadc807d602d5aa1ec0b56c10e80d54a9ea97b01ccbf7e9cef5ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
