{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "from kaggle_environments import make\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Envrionment parameters\n",
    "cols = 7\n",
    "rows = 6\n",
    "\n",
    "env = make(\"connectx\", configuration={\"rows\":rows, \"columns\":cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define some hyperparameters and the agent's architecture\n",
    "\n",
    "\n",
    "observation_space = gym.spaces.Discrete(cols * rows)\n",
    "action_space = gym.spaces.Discrete(cols)\n",
    "\n",
    "\n",
    "EPISODES = 150\n",
    "LR = 0.0001\n",
    "MEM_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.999\n",
    "EXPLORATION_MIN = 0.001\n",
    "sync_freq = 10\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = (cols*rows,)\n",
    "        self.action_space = action_space.n\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_shape, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "        self.loss = nn.MSELoss()\n",
    "        #self.to(DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEM_SIZE)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "        state1_batch = torch.stack([s1 for (s1,a,r,s2,d) in minibatch])\n",
    "        action_batch = torch.tensor([a for (s1,a,r,s2,d) in minibatch])\n",
    "        reward_batch = torch.tensor([r for (s1,a,r,s2,d) in minibatch])\n",
    "        state2_batch = torch.stack([s2 for (s1,a,r,s2,d) in minibatch])\n",
    "        done_batch = torch.tensor([d for (s1,a,r,s2,d) in minibatch])\n",
    "\n",
    "        return (state1_batch, action_batch, reward_batch, state2_batch, done_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.replay = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network()\n",
    "        self.network2 = copy.deepcopy(self.network) #A\n",
    "        self.network2.load_state_dict(self.network.state_dict())\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return action_space.sample()\n",
    "\n",
    "        # Convert observation to PyTorch Tensor\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        #state = state.to(DEVICE)\n",
    "        state = state.unsqueeze(0)\n",
    "            \n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        # Get Q(s,.)\n",
    "        q_values = self.network(state)\n",
    "\n",
    "        # Choose the action to play\n",
    "        action = torch.argmax(q_values).item()\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay.memory)< BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        # Sample minibatch s1, a1, r1, s1', done_1, ... , sn, an, rn, sn', done_n\n",
    "        state1_batch, action_batch, reward_batch, state2_batch, done_batch = self.replay.sample()\n",
    "\n",
    "        # Compute Q values\n",
    "        q_values = self.network(state1_batch).squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute next Q values\n",
    "            next_q_values = self.network2(state2_batch).squeeze()\n",
    "\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        predicted_value_of_now = q_values[batch_indices, action_batch]\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "\n",
    "        # Compute the q_target\n",
    "        q_target = reward_batch + GAMMA * predicted_value_of_future * (1-(done_batch).long())\n",
    "\n",
    "        # Compute the loss (c.f. self.network.loss())\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        # Complute 𝛁Q\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m average_reward_number \u001b[39m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m j\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, EPISODES)):\n\u001b[1;32m     11\u001b[0m     state, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     12\u001b[0m     state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(state, [\u001b[39m1\u001b[39m, observation_space])\n",
      "File \u001b[0;32m~/miniconda3/envs/puissance4/lib/python3.10/site-packages/tqdm/notebook.py:243\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    242\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[0;32m--> 243\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[1;32m    244\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/puissance4/lib/python3.10/site-packages/tqdm/notebook.py:118\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[1;32m    120\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "agent = DQN()\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "j=0\n",
    "for i in tqdm(range(1, EPISODES)):\n",
    "    state, info = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        j+=1\n",
    "\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, truncated, info = env.step(action)\n",
    "        state_ = np.reshape(state_, [1, observation_space])\n",
    "        state = torch.tensor(state).float()\n",
    "        state_ = torch.tensor(state_).float()\n",
    "\n",
    "        exp = (state, action, reward, state_, done)\n",
    "        agent.replay.add(exp)\n",
    "        agent.learn()\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if j % sync_freq == 0:\n",
    "            agent.network2.load_state_dict(agent.network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            if i%10==0:\n",
    "                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "                #test_model(agent,10, observation_space)\n",
    "            break\n",
    "  \n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n",
    "\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "puissance4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbc9bc058a47547e04c127d14e4ddbe2a6ee888f45573976dab25d4d6ce9c846"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
